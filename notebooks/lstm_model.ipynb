{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162d3538",
   "metadata": {},
   "source": [
    "## Problem Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd2b41",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0a223",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import ydata_profiling\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import mlflow\n",
    "\n",
    "# Configurar pandas para mostrar todas las columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6364e8",
   "metadata": {},
   "source": [
    "### Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb631369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Ingesta y unificación de datos con polars\n",
    "\n",
    "# Rutas de los archivos\n",
    "setA_path = r\"C:\\repos\\physionet-sepsis-forecasting\\data\\raw\\all_patients_setA.parquet\"\n",
    "setB_path = r\"C:\\repos\\physionet-sepsis-forecasting\\data\\raw\\all_patients_setB.parquet\"\n",
    "unified_path = r\"C:\\repos\\physionet-sepsis-forecasting\\data\\raw\\all_patients_unified.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ca462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer ambos datasets con pandas\n",
    "df_a = pd.read_parquet(setA_path)\n",
    "df_b = pd.read_parquet(setB_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar las columnas que NO tienen en común\n",
    "print(\"Columnas en Set A pero no en Set B:\", set(df_a.columns) - set(df_b.columns))\n",
    "print(\"Columnas en Set B pero no en Set A:\", set(df_b.columns) - set(df_a.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc51889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que ambos tengan las mismas columnas eliminando las que no coinciden\n",
    "common_cols = list(set(df_a.columns) & set(df_b.columns))\n",
    "df_a = df_a[common_cols]\n",
    "df_b = df_b[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ada992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unificar\n",
    "df = pd.concat([df_a, df_b])\n",
    "\n",
    "# Guardar el dataset unificado\n",
    "df.to_parquet(unified_path)\n",
    "print(f\"Dataset unificado guardado en {unified_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar memoria eliminando df_a y df_b\n",
    "del df_a\n",
    "del df_b\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939be8fa",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Análisis exploratorio\n",
    "df = pd.read_parquet(unified_path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fd334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88311f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción estadística rápida\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf338a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de valores nulos por columna con porcentaje\n",
    "null_counts = df.isnull().sum()\n",
    "null_percent = (null_counts / len(df)) * 100\n",
    "null_df = pd.DataFrame({'null_count': null_counts, 'null_percent': null_percent})\n",
    "null_df = null_df[null_df['null_count'] >= 0].sort_values(by='null_percent', ascending=False)\n",
    "print(null_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019576d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar desbalance de clases en la variable objetivo SepsisLabel\n",
    "sepsis_counts = df['SepsisLabel'].value_counts()\n",
    "sepsis_percent = (sepsis_counts / len(df)) * 100\n",
    "sepsis_classes = pd.DataFrame({'sepsis_counts': sepsis_counts, 'sepsis_percent': sepsis_percent})\n",
    "sepsis_classes = sepsis_classes[sepsis_classes['sepsis_counts'] >= 0].sort_values(by='sepsis_percent', ascending=False)\n",
    "\n",
    "print(sepsis_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar un reporte con ydata-profiling\n",
    "\n",
    "profile = ydata_profiling.ProfileReport(df, title=\"Reporte de Análisis Exploratorio\", explorative=True, minimal=True)\n",
    "# Si el directorio no existe, crearlo\n",
    "os.makedirs(os.path.dirname(r\"C:\\repos\\physionet-sepsis-forecasting\\data\\reports\"), exist_ok=True)\n",
    "# Guardar el reporte\n",
    "profile_path = r\"C:\\repos\\physionet-sepsis-forecasting\\data\\reports\\eda_report.html\"\n",
    "profile.to_file(profile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d2467",
   "metadata": {},
   "source": [
    "## Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc7e4a",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar valores nulos con la media usando pandas y para las variables categóricas con la moda\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    mean_value = df[col].mean()\n",
    "    df[col] = df[col].fillna(mean_value)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    mode_value = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bc59e",
   "metadata": {},
   "source": [
    "### Data Scalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar StandardScaler de sklearn a las columnas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Excluir columnas que no deben ser escaladas\n",
    "exclude_cols = ['SepsisLabel', 'patient_id', 'ICULOS']\n",
    "numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7f206",
   "metadata": {},
   "source": [
    "## Modeling a LSTM\n",
    "\n",
    "Transformamos el dataframe (que tiene una fila por hora por paciente) en un formato (número_de_muestras, longitud_de_secuencia, número_de_características), que es lo que un LSTM espera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab15f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenamos y agrupamos por paciente\n",
    "df = df.sort_values(by=['patient_id', 'ICULOS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198159d",
   "metadata": {},
   "source": [
    "### Define features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denifimos características y etiquetas\n",
    "features_cols = [col for col in df.columns if col not in ['SepsisLabel', 'patient_id']]\n",
    "target_col = 'SepsisLabel'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b7f8e",
   "metadata": {},
   "source": [
    "### Sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ac587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la secuencia minima y máxima por paciente\n",
    "seq_lengths = df.groupby('patient_id').size()\n",
    "min_seq_length = seq_lengths.min()\n",
    "max_seq_length = seq_lengths.max() \n",
    "print(f\"Longitud mínima de secuencia por paciente: {min_seq_length}\")\n",
    "print(f\"Longitud máxima de secuencia por paciente: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar cuantos pacientes hay con la secuencia mínima y máxima\n",
    "min_seq_count = (seq_lengths == min_seq_length).sum()\n",
    "max_seq_count = (seq_lengths == max_seq_length).sum()\n",
    "print(f\"Número de pacientes con la secuencia mínima ({min_seq_length}): {min_seq_count}\")\n",
    "print(f\"Número de pacientes con la secuencia máxima ({max_seq_length}): {max_seq_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos secuencias para LSTM (Ventanas deslizantes)\n",
    "# Parámetros\n",
    "sequence_length = min_seq_length # Usar la secuencia minima de datos como para predecir la siguiente\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "# Agrupar por paciente para no mezclar datos de diferentes personas\n",
    "grouped = df.groupby('patient_id')\n",
    "\n",
    "for _, group in grouped:\n",
    "    features = group[features_cols].values\n",
    "    target = group[target_col].values\n",
    "    \n",
    "    # Crear ventanas deslizantes para cada paciente\n",
    "    for i in range(len(group) - sequence_length):\n",
    "        X_sequences.append(features[i:i + sequence_length])\n",
    "        y_sequences.append(target[i + sequence_length])\n",
    "\n",
    "# Convertir a arrays de NumPy\n",
    "X = np.array(X_sequences)\n",
    "y = np.array(y_sequences)\n",
    "\n",
    "print(f\"Forma de las secuencias de entrada (X): {X.shape}\")\n",
    "print(f\"Forma de las etiquetas de salida (y): {y.shape}\")\n",
    "\n",
    "# La salida de X.shape debería ser (num_muestras, min_seq_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513a4bc",
   "metadata": {},
   "source": [
    "### Split in Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train shapes: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation shapes: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test shapes: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c0034",
   "metadata": {},
   "source": [
    "### Save processed data\n",
    "\n",
    "Ahora que tenemos los datos en el formato correcto los guardaremos en la carpeta `data\\processed`. No guardaremos un .parquet porque ya no es un dataframe 2D. Usaremos el formato de NumPy (.npy) que es ideal para arrays multidimensionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdfd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = r\"C:\\repos\\physionet-sepsis-forecasting\\data\\processed\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(processed_dir, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(processed_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(processed_dir, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(processed_dir, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(processed_dir, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(processed_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(\"Datos procesados y divididos guardados en data/processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae887b3",
   "metadata": {},
   "source": [
    "### Versionamiento con DVC:\n",
    "Ahora, añade estos archivos .npy a DVC, tal como hiciste con el archivo Parquet, y sube los cambios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc add data/processed\n",
    "git add data/processed.dvc\n",
    "git commit -m \"feat: Create time-series sequences for LSTM model\"\n",
    "dvc push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6993819",
   "metadata": {},
   "source": [
    "### Fase 2: Implementación y Entrenamiento del Modelo LSTM\n",
    "\n",
    "**Objetivo**: Construir la arquitectura del modelo, entrenarlo con los datos secuenciales y registrar los resultados con MLflow.\n",
    "\n",
    "2.1. Adaptar el Notebook para el Entrenamiento (o crear scripts/train.py):\n",
    "Te recomiendo encarecidamente mover el código de entrenamiento del notebook a un script scripts/train.py para seguir las buenas prácticas del repositorio.\n",
    "2.2. Crear los DataLoaders de PyTorch:\n",
    "PyTorch usa DataLoader para gestionar los datos en batches de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memoria de las variables originales\n",
    "del X_train, y_train, X_val, y_val, X_test, y_test, df, X_temp, y_temp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db982ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos desde data/processed\n",
    "processed_dir = r\"C:\\repos\\physionet-sepsis-forecasting\\data\\processed\"\n",
    "X_train = np.load(os.path.join(processed_dir, 'X_train.npy'))\n",
    "y_train = np.load(os.path.join(processed_dir, 'y_train.npy'))\n",
    "X_val = np.load(os.path.join(processed_dir, 'X_val.npy'))\n",
    "y_val = np.load(os.path.join(processed_dir, 'y_val.npy'))\n",
    "\n",
    "# Convertir a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Crear TensorDatasets y DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "\n",
    "num_workers = os.cpu_count()  # Usa todos los núcleos disponibles\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8761db",
   "metadata": {},
   "source": [
    "### 2.3. Definir la Arquitectura del Modelo LSTM:\n",
    "Aquí definimos las capas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SepsisLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(SepsisLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Inicializar estados ocultos\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Pasar por el LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Tomar la salida del último paso de tiempo y pasarla por la capa densa\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34eb21",
   "metadata": {},
   "source": [
    "### 2.4. Bucle de Entrenamiento y Evaluación:\n",
    "Este es el corazón del entrenamiento, donde iteramos sobre los datos, calculamos la pérdida y ajustamos los pesos del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar solo el 5% del dataset de entrenamiento para pruebas rápidas\n",
    "sample_frac = 0.05\n",
    "num_samples = int(X_train_tensor.shape[0] * sample_frac)\n",
    "\n",
    "X_train_small = X_train_tensor[:num_samples]\n",
    "y_train_small = y_train_tensor[:num_samples]\n",
    "\n",
    "train_dataset_small = TensorDataset(X_train_small, y_train_small)\n",
    "train_loader_small = DataLoader(train_dataset_small, batch_size=64, shuffle=True, num_workers=os.cpu_count())\n",
    "\n",
    "# Configuración del modelo y entrenamiento rápido\n",
    "input_size = X_train_small.shape[2] # Número de features\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SepsisLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy para clasificación binaria\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- INTEGRACIÓN CON MLFLOW ---\n",
    "mlflow.set_tracking_uri(\"http://100.24.7.21:5000\")\n",
    "with mlflow.start_run(nested=True) as run:\n",
    "    mlflow.log_params({\"hidden_size\": hidden_size, \"num_layers\": num_layers, \"epochs\": num_epochs, \"train_frac\": sample_frac})\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader_small:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Bucle de validación (calcular métricas como AUC-ROC en el val_loader)\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val AUC: {auc:.4f}\")\n",
    "        mlflow.log_metric(\"val_auc\", auc, step=epoch)\n",
    "\n",
    "        mlflow.pytorch.log_model(model, \"lst_model_8_sw_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09544720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo y entrenamiento\n",
    "input_size = X_train_tensor.shape[2] # Número de features\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SepsisLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy para clasificación binaria\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- INTEGRACIÓN CON MLFLOW ---\n",
    "mlflow.set_tracking_uri(\"http://100.24.7.21:5000\")\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_params({\"hidden_size\": hidden_size, \"num_layers\": num_layers, \"epochs\": num_epochs})\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Bucle de validación (calcular métricas como AUC-ROC en el val_loader)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val AUC: {auc:.4f}\")\n",
    "    mlflow.log_metric(\"val_auc\", auc, step=epoch)\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"lst_model_8_sw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo y entrenamiento\n",
    "input_size = X_train.shape[2] # Número de features\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SepsisLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy para clasificación binaria\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- INTEGRACIÓN CON MLFLOW ---\n",
    "mlflow.set_tracking_uri(\"http://54.226.120.46:5000\")\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_params({\"hidden_size\": hidden_size, \"num_layers\": num_layers, \"epochs\": num_epochs})\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Bucle de validación (calcular métricas como AUC-ROC en el val_loader)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val AUC: {auc:.4f}\")\n",
    "    mlflow.log_metric(\"val_auc\", auc, step=epoch)\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"lst_model_8_sw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1117cac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sepsis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
